{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPBF Optimizer Workflow\n",
    "\n",
    "This notebook demonstrates the complete workflow of the LPBF Optimizer, from data generation to optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure paths\n",
    "config_path = '../data/params.yaml'\n",
    "data_dir = Path('../data')\n",
    "processed_dir = data_dir / 'processed'\n",
    "models_dir = data_dir / 'models'\n",
    "results_dir = data_dir / 'optimized'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Data\n",
    "\n",
    "First, let's generate synthetic data that mimics FEA simulation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_synthetic_data import SyntheticDataGenerator\n",
    "\n",
    "# Initialize data generator\n",
    "generator = SyntheticDataGenerator(config_path)\n",
    "\n",
    "# Generate synthetic dataset\n",
    "# For demonstration, we use fewer scan vectors and points than a real application\n",
    "dataset_path = generator.generate(n_scan_vectors=20, n_points_per_vector=100)\n",
    "\n",
    "print(f\"Generated synthetic dataset at: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Explore the Generated Data\n",
    "\n",
    "Let's examine the synthetic dataset to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the generated dataset\n",
    "with h5py.File(dataset_path, 'r') as f:\n",
    "    # Print dataset structure\n",
    "    print(\"Dataset structure:\")\n",
    "    def print_structure(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"  {name}: shape={obj.shape}, dtype={obj.dtype}\")\n",
    "    f.visititems(print_structure)\n",
    "    \n",
    "    # Load some data for visualization\n",
    "    train_outputs = f['train/outputs'][:]\n",
    "    param_names = [name.decode('utf-8') for name in f['metadata/parameter_names'][:]]\n",
    "    train_scan_vectors = f['train/scan_vectors'][:]\n",
    "    \n",
    "    # Get metadata\n",
    "    n_train = f['metadata'].attrs['n_train']\n",
    "    n_val = f['metadata'].attrs['n_val']\n",
    "    n_test = f['metadata'].attrs['n_test']\n",
    "    \n",
    "print(f\"\\nParameter names: {param_names}\")\n",
    "print(f\"Dataset split: {n_train} training, {n_val} validation, {n_test} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize some of the outputs from the synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot outputs from the first 100 samples\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot residual stress\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(train_outputs[:100, 0], bins=20)\n",
    "plt.title('Residual Stress Distribution')\n",
    "plt.xlabel('Stress (MPa)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot porosity\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(train_outputs[:100, 1], bins=20)\n",
    "plt.title('Porosity Distribution')\n",
    "plt.xlabel('Porosity (%)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Plot geometric accuracy\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.hist(train_outputs[:100, 2], bins=20)\n",
    "plt.title('Geometric Accuracy Distribution')\n",
    "plt.xlabel('Accuracy Ratio')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Examine relationships between parameters and outputs\n",
    "if len(param_names) >= 2:  # If we have at least laser power and scan speed\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Power vs. Stress\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(train_scan_vectors[:50, 0], train_outputs[:50, 0])\n",
    "    plt.title(f'{param_names[0]} vs. Residual Stress')\n",
    "    plt.xlabel(f'{param_names[0]}')\n",
    "    plt.ylabel('Residual Stress (MPa)')\n",
    "    \n",
    "    # Speed vs. Porosity\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(train_scan_vectors[:50, 1], train_outputs[:50, 1])\n",
    "    plt.title(f'{param_names[1]} vs. Porosity')\n",
    "    plt.xlabel(f'{param_names[1]}')\n",
    "    plt.ylabel('Porosity (%)')\n",
    "    \n",
    "    # Power-to-Speed Ratio vs. Accuracy\n",
    "    energy_density = train_scan_vectors[:50, 0] / train_scan_vectors[:50, 1]\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(energy_density, train_outputs[:50, 2])\n",
    "    plt.title('Energy Density vs. Accuracy')\n",
    "    plt.xlabel('Energy Density (P/v)')\n",
    "    plt.ylabel('Geometric Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the PINN Model\n",
    "\n",
    "Now let's train a Physics-Informed Neural Network using this synthetic data. We'll do a simplified training for demonstration purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinn.model import PINN\n",
    "from pinn.physics import compute_physics_loss\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load configuration\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Load a small portion of the data for demonstration\n",
    "with h5py.File(dataset_path, 'r') as f:\n",
    "    # Load inputs and outputs\n",
    "    train_inputs = torch.tensor(f['train/inputs'][:200], dtype=torch.float32)\n",
    "    train_outputs = torch.tensor(f['train/outputs'][:200], dtype=torch.float32)\n",
    "    train_coords = torch.tensor(f['train/coordinates'][:200], dtype=torch.float32)\n",
    "    train_time = torch.tensor(f['train/time'][:200], dtype=torch.float32)\n",
    "    train_scan_vectors = torch.tensor(f['train/scan_vectors'][:], dtype=torch.float32)\n",
    "    \n",
    "    # Get unique scan vectors (for demo, just use a few)\n",
    "    unique_scan_vectors = train_scan_vectors[:5]\n",
    "    \n",
    "    # Find indices for these scan vectors in the inputs\n",
    "    n_scan_params = unique_scan_vectors.shape[1]\n",
    "    selected_indices = []\n",
    "    for i in range(len(train_inputs)):\n",
    "        input_sv = train_inputs[i, :n_scan_params]\n",
    "        if any(torch.all(input_sv == sv) for sv in unique_scan_vectors):\n",
    "            selected_indices.append(i)\n",
    "        if len(selected_indices) >= 100:  # Limit to 100 samples for demo\n",
    "            break\n",
    "    \n",
    "    # Select the data\n",
    "    inputs = train_inputs[selected_indices]\n",
    "    outputs = train_outputs[selected_indices]\n",
    "    coords = train_coords[selected_indices]\n",
    "    time = train_time[selected_indices]\n",
    "    \n",
    "# Create and train model\n",
    "model_config = config['model']\n",
    "model = PINN(\n",
    "    in_dim=model_config['input_dim'],\n",
    "    out_dim=model_config['output_dim'],\n",
    "    width=model_config['hidden_width'],\n",
    "    depth=model_config['hidden_depth']\n",
    ")\n",
    "\n",
    "# Define loss and optimizer\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Simple training loop (for demonstration)\n",
    "n_epochs = 20  # Just a few epochs for demonstration\n",
    "losses = []\n",
    "\n",
    "print(\"Starting demo training loop...\")\n",
    "for epoch in range(n_epochs):\n",
    "    # Forward pass\n",
    "    optimizer.zero_grad()\n",
    "    predictions = model(inputs)\n",
    "    \n",
    "    # Data loss\n",
    "    data_loss = mse_loss(predictions, outputs)\n",
    "    \n",
    "    # Physics loss (simplified for demo)\n",
    "    physics_loss = compute_physics_loss(\n",
    "        model, \n",
    "        inputs[:, :n_scan_params], \n",
    "        coords, \n",
    "        time, \n",
    "        config['material_properties'],\n",
    "        lambda_heat=config['training']['lambda_heat'],\n",
    "        lambda_stress=config['training']['lambda_stress']\n",
    "    )\n",
    "    \n",
    "    # Total loss\n",
    "    total_loss = data_loss + physics_loss\n",
    "    \n",
    "    # Backward and optimize\n",
    "    total_loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    losses.append(total_loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}], Loss: {total_loss.item():.4f}, \"\n",
    "              f\"Data Loss: {data_loss.item():.4f}, Physics Loss: {physics_loss.item():.4f}\")\n",
    "\n",
    "# Save the model for later use\n",
    "model_path = models_dir / 'demo_model.pt'\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'loss': total_loss.item()\n",
    "}, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(losses)\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make Predictions with the Trained Model\n",
    "\n",
    "Let's use our trained model to make some predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some new scan vectors for prediction\n",
    "with h5py.File(dataset_path, 'r') as f:\n",
    "    param_bounds = {}\n",
    "    param_names = [name.decode('utf-8') for name in f['metadata/parameter_names'][:]]\n",
    "    for i, param in enumerate(param_names):\n",
    "        # Extract bounds from existing data\n",
    "        values = f['train/scan_vectors'][:, i]\n",
    "        param_bounds[param] = [np.min(values), np.max(values)]\n",
    "\n",
    "# Create a grid of test points for the first two parameters\n",
    "if len(param_names) >= 2:\n",
    "    param1 = param_names[0]  # e.g., Laser power\n",
    "    param2 = param_names[1]  # e.g., Scan speed\n",
    "    \n",
    "    p1_range = np.linspace(param_bounds[param1][0], param_bounds[param1][1], 10)\n",
    "    p2_range = np.linspace(param_bounds[param2][0], param_bounds[param2][1], 10)\n",
    "    P1, P2 = np.meshgrid(p1_range, p2_range)\n",
    "    \n",
    "    # Create test scan vectors\n",
    "    test_points = []\n",
    "    for i in range(P1.shape[0]):\n",
    "        for j in range(P1.shape[1]):\n",
    "            point = [P1[i,j], P2[i,j]]\n",
    "            # Add default values for other parameters\n",
    "            for k in range(2, len(param_names)):\n",
    "                point.append(np.mean(param_bounds[param_names[k]]))\n",
    "            test_points.append(point)\n",
    "    \n",
    "    test_scan_vectors = torch.tensor(test_points, dtype=torch.float32)\n",
    "    \n",
    "    # Add dummy coordinates and time for prediction\n",
    "    coords = torch.zeros(len(test_scan_vectors), 3)  # Origin point (0,0,0)\n",
    "    time = torch.ones(len(test_scan_vectors), 1)     # Final time (t=1)\n",
    "    \n",
    "    # Combine inputs for the model\n",
    "    test_inputs = torch.cat([test_scan_vectors, coords, time], dim=1)\n",
    "    \n",
    "    # Make predictions\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(test_inputs)\n",
    "    \n",
    "    # Reshape for visualization\n",
    "    stress = predictions[:, 0].reshape(P1.shape)\n",
    "    porosity = predictions[:, 1].reshape(P1.shape)\n",
    "    accuracy = predictions[:, 2].reshape(P1.shape)\n",
    "    \n",
    "    # Visualize the results\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot residual stress\n",
    "    plt.subplot(1, 3, 1)\n",
    "    contour = plt.contourf(P1, P2, stress, cmap='hot')\n",
    "    plt.colorbar(contour)\n",
    "    plt.title('Residual Stress')\n",
    "    plt.xlabel(param1)\n",
    "    plt.ylabel(param2)\n",
    "    \n",
    "    # Plot porosity\n",
    "    plt.subplot(1, 3, 2)\n",
    "    contour = plt.contourf(P1, P2, porosity, cmap='viridis')\n",
    "    plt.colorbar(contour)\n",
    "    plt.title('Porosity')\n",
    "    plt.xlabel(param1)\n",
    "    plt.ylabel(param2)\n",
    "    \n",
    "    # Plot geometric accuracy\n",
    "    plt.subplot(1, 3, 3)\n",
    "    contour = plt.contourf(P1, P2, accuracy, cmap='Blues')\n",
    "    plt.colorbar(contour)\n",
    "    plt.title('Geometric Accuracy')\n",
    "    plt.xlabel(param1)\n",
    "    plt.ylabel(param2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-objective Optimization\n",
    "\n",
    "Now let's use the NSGA-III algorithm to find optimal process parameters based on our trained surrogate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.core.problem import Problem\n",
    "from pymoo.algorithms.moo.nsga3 import NSGA3\n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.factory import get_sampling, get_crossover, get_mutation\n",
    "\n",
    "class SurrogateProblem(Problem):\n",
    "    \"\"\"\n",
    "    Multi-objective optimization problem using PINN surrogate\n",
    "    \"\"\"\n",
    "    def __init__(self, model, param_bounds, objectives):\n",
    "        # Extract parameter bounds\n",
    "        self.n_var = len(param_bounds)\n",
    "        self.param_names = list(param_bounds.keys())\n",
    "        self.xl = np.array([param_bounds[p][0] for p in self.param_names])\n",
    "        self.xu = np.array([param_bounds[p][1] for p in self.param_names])\n",
    "        \n",
    "        # Set objectives\n",
    "        self.n_obj = len(objectives)\n",
    "        self.objectives = objectives\n",
    "        \n",
    "        # The optimization is unconstrained\n",
    "        self.n_constr = 0\n",
    "        \n",
    "        # Store the surrogate model\n",
    "        self.model = model\n",
    "        \n",
    "        # Call parent constructor\n",
    "        super().__init__(\n",
    "            n_var=self.n_var,\n",
    "            n_obj=self.n_obj,\n",
    "            n_constr=self.n_constr,\n",
    "            xl=self.xl,\n",
    "            xu=self.xu\n",
    "        )\n",
    "    \n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        # Convert to tensor\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32)\n",
    "        \n",
    "        # Add dummy spatial coordinates and time\n",
    "        batch_size = x_tensor.shape[0]\n",
    "        coords = torch.zeros(batch_size, 3)  # Origin point\n",
    "        time = torch.ones(batch_size, 1)     # Final time step\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            model_input = torch.cat([x_tensor, coords, time], dim=1)\n",
    "            predictions = self.model(model_input)\n",
    "        \n",
    "        # Extract objective values\n",
    "        objectives = predictions.numpy()\n",
    "        \n",
    "        # For minimization, we negate geometric accuracy (higher is better)\n",
    "        objectives[:, 2] = -objectives[:, 2]  # Negate accuracy for minimization\n",
    "        \n",
    "        # Set the objective values for pymoo\n",
    "        out[\"F\"] = objectives\n",
    "\n",
    "# Run a simple optimization\n",
    "if len(param_names) >= 2:\n",
    "    # Define the problem\n",
    "    problem = SurrogateProblem(\n",
    "        model=model,\n",
    "        param_bounds=param_bounds,\n",
    "        objectives=['residual_stress', 'porosity', 'geometric_accuracy']\n",
    "    )\n",
    "    \n",
    "    # Get reference directions\n",
    "    ref_dirs = get_reference_directions(\"das-dennis\", 3, n_partitions=4)\n",
    "    \n",
    "    # Create the algorithm\n",
    "    algorithm = NSGA3(\n",
    "        pop_size=40,\n",
    "        ref_dirs=ref_dirs,\n",
    "        sampling=get_sampling(\"real_random\"),\n",
    "        crossover=get_crossover(\"real_sbx\", prob=0.9, eta=15),\n",
    "        mutation=get_mutation(\"real_pm\", eta=20),\n",
    "        eliminate_duplicates=True\n",
    "    )\n",
    "    \n",
    "    # Run the optimization\n",
    "    print(\"Running NSGA-III optimization (with limited generations for demo)...\")\n",
    "    res = minimize(\n",
    "        problem=problem,\n",
    "        algorithm=algorithm,\n",
    "        termination=('n_gen', 20),  # Limited generations for demo\n",
    "        seed=1,\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Get the Pareto optimal solutions\n",
    "    X = res.X  # Optimal process parameters\n",
    "    F = res.F  # Corresponding objective values\n",
    "    \n",
    "    # Save results\n",
    "    results_path = results_dir / 'demo_pareto_solutions.csv'\n",
    "    import pandas as pd\n",
    "    param_df = pd.DataFrame(X, columns=problem.param_names)\n",
    "    obj_df = pd.DataFrame(F, columns=['residual_stress', 'porosity', '-geometric_accuracy'])\n",
    "    result_df = pd.concat([param_df, obj_df], axis=1)\n",
    "    result_df.to_csv(results_path, index=False)\n",
    "    \n",
    "    print(f\"Optimization complete! Results saved to {results_path}\")\n",
    "    print(f\"Found {len(X)} Pareto-optimal solutions\")\n",
    "    \n",
    "    # Visualize Pareto front\n",
    "    if F.shape[1] == 3:\n",
    "        fig = plt.figure(figsize=(10, 8))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        ax.scatter(F[:, 0], F[:, 1], F[:, 2], s=30)\n",
    "        ax.set_xlabel('Residual Stress')\n",
    "        ax.set_ylabel('Porosity')\n",
    "        ax.set_zlabel('-Geometric Accuracy')\n",
    "        ax.set_title('Pareto Front')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This notebook has demonstrated the complete workflow for the LPBF Optimizer:\n",
    "\n",
    "1. Generating synthetic data that mimics FEA simulations\n",
    "2. Exploring and visualizing the data\n",
    "3. Training a physics-informed neural network (PINN)\n",
    "4. Making predictions using the trained model\n",
    "5. Running multi-objective optimization to find optimal process parameters\n",
    "\n",
    "In a real application, you would:\n",
    "- Generate or use much larger datasets\n",
    "- Train the model for many more epochs\n",
    "- Run the optimization with a larger population size and more generations\n",
    "- Validate the results with actual LPBF builds\n",
    "\n",
    "For more information, see the documentation in the README.md file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}