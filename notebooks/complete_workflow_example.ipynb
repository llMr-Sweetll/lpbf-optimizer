{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LPBF Optimizer: Complete Workflow Example\n",
    "\n",
    "This notebook demonstrates the complete end-to-end workflow for LPBF process optimization using physics-informed neural networks and multi-objective optimization. The workflow includes:\n",
    "\n",
    "1. Generating synthetic data (simulating FEA results)\n",
    "2. Data preprocessing and exploration\n",
    "3. Training a physics-informed neural network (PINN)\n",
    "4. Using the PINN as a surrogate model for multi-objective optimization\n",
    "5. Analyzing the Pareto-optimal solutions\n",
    "6. Visualizing process-property relationships"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary libraries and set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import torch\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Add the src directory to the Python path\n",
    "sys.path.append(os.path.abspath('../src'))\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Configure paths\n",
    "config_path = '../data/params.yaml'\n",
    "data_dir = Path('../data')\n",
    "processed_dir = data_dir / 'processed'\n",
    "models_dir = data_dir / 'models'\n",
    "results_dir = data_dir / 'optimized'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "models_dir.mkdir(parents=True, exist_ok=True)\n",
    "results_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load configuration\n",
    "with open(config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "    \n",
    "# Set up matplotlib for better visualization\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synthetic Data Generation\n",
    "\n",
    "We'll generate synthetic data that mimics FEA simulation results for LPBF process outcomes. This simulates running hundreds of finite element analyses for different process parameter combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from generate_synthetic_data import SyntheticDataGenerator\n",
    "\n",
    "# Initialize data generator\n",
    "generator = SyntheticDataGenerator(config_path)\n",
    "\n",
    "# For this demo, we'll use a smaller dataset\n",
    "n_scan_vectors = 50\n",
    "n_points_per_vector = 200\n",
    "\n",
    "print(f\"Generating dataset with {n_scan_vectors} scan vectors and {n_points_per_vector} points per vector...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Generate the synthetic dataset\n",
    "dataset_path = generator.generate(n_scan_vectors=n_scan_vectors, n_points_per_vector=n_points_per_vector)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Generation complete in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Dataset saved to: {dataset_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "Now let's explore the dataset to understand its structure and the relationships between process parameters and outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the dataset\n",
    "with h5py.File(dataset_path, 'r') as f:\n",
    "    # Print dataset structure\n",
    "    print(\"Dataset structure:\")\n",
    "    def print_structure(name, obj):\n",
    "        if isinstance(obj, h5py.Dataset):\n",
    "            print(f\"  {name}: shape={obj.shape}, dtype={obj.dtype}\")\n",
    "    f.visititems(print_structure)\n",
    "    \n",
    "    # Get metadata\n",
    "    param_names = [name.decode('utf-8') for name in f['metadata/parameter_names'][:]]\n",
    "    n_train = f['metadata'].attrs['n_train']\n",
    "    n_val = f['metadata'].attrs['n_val']\n",
    "    n_test = f['metadata'].attrs['n_test']\n",
    "    \n",
    "    # Load sample data for visualization\n",
    "    # Use unique scan vectors to see parameter distributions\n",
    "    train_scan_vectors = f['train/scan_vectors'][:]\n",
    "    train_outputs = f['train/outputs'][:1000]  # Sample of the outputs\n",
    "    \n",
    "print(f\"\\nParameter names: {param_names}\")\n",
    "print(f\"Dataset split: {n_train} training, {n_val} validation, {n_test} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Visualize Process Parameter Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of process parameters\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i, param in enumerate(param_names[:4]):  # Show first 4 parameters\n",
    "    if i < len(axes):\n",
    "        axes[i].hist(train_scan_vectors[:, i], bins=20, alpha=0.7)\n",
    "        axes[i].set_title(f'Distribution of {param}')\n",
    "        axes[i].set_xlabel(param)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Visualize Process Outcome Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of outputs\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "outcome_names = ['Residual Stress (MPa)', 'Porosity (%)', 'Geometric Accuracy Ratio']\n",
    "\n",
    "for i, name in enumerate(outcome_names):\n",
    "    axes[i].hist(train_outputs[:, i], bins=30, alpha=0.7)\n",
    "    axes[i].set_title(f'Distribution of {name}')\n",
    "    axes[i].set_xlabel(name)\n",
    "    axes[i].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Explore Process-Property Relationships\n",
    "\n",
    "Let's look at how key process parameters affect the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load more data for relationship exploration\n",
    "with h5py.File(dataset_path, 'r') as f:\n",
    "    # Get a subset of inputs and outputs\n",
    "    inputs = f['train/inputs'][:2000]\n",
    "    outputs = f['train/outputs'][:2000]\n",
    "    coordinates = f['train/coordinates'][:2000]\n",
    "    n_params = len(param_names)\n",
    "    \n",
    "    # Extract parameters portion of inputs\n",
    "    params = inputs[:, :n_params]\n",
    "\n",
    "# Create a figure with scatter plots showing relationships\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "\n",
    "# Laser power vs. Residual stress\n",
    "axes[0, 0].scatter(params[:, 0], outputs[:, 0], alpha=0.5, c=params[:, 1], cmap='viridis')\n",
    "axes[0, 0].set_title('Laser Power vs. Residual Stress')\n",
    "axes[0, 0].set_xlabel('Laser Power (W)')\n",
    "axes[0, 0].set_ylabel('Residual Stress (MPa)')\n",
    "axes[0, 0].colorbar = plt.colorbar(axes[0, 0].collections[0], ax=axes[0, 0])\n",
    "axes[0, 0].colorbar.set_label('Scan Speed (mm/s)')\n",
    "\n",
    "# Scan speed vs. Porosity\n",
    "axes[0, 1].scatter(params[:, 1], outputs[:, 1], alpha=0.5, c=params[:, 0], cmap='plasma')\n",
    "axes[0, 1].set_title('Scan Speed vs. Porosity')\n",
    "axes[0, 1].set_xlabel('Scan Speed (mm/s)')\n",
    "axes[0, 1].set_ylabel('Porosity (%)')\n",
    "axes[0, 1].colorbar = plt.colorbar(axes[0, 1].collections[0], ax=axes[0, 1])\n",
    "axes[0, 1].colorbar.set_label('Laser Power (W)')\n",
    "\n",
    "# Energy density vs. Geometric accuracy\n",
    "energy_density = params[:, 0] / (params[:, 1] * params[:, 2]) # P/(v*h)\n",
    "scatter = axes[0, 2].scatter(energy_density, outputs[:, 2], alpha=0.5, c=params[:, 3], cmap='coolwarm')\n",
    "axes[0, 2].set_title('Energy Density vs. Geometric Accuracy')\n",
    "axes[0, 2].set_xlabel('Energy Density (J/mm³)')\n",
    "axes[0, 2].set_ylabel('Geometric Accuracy Ratio')\n",
    "axes[0, 2].colorbar = plt.colorbar(scatter, ax=axes[0, 2])\n",
    "axes[0, 2].colorbar.set_label('Scan Angle (°)')\n",
    "\n",
    "# Hatch spacing vs. Residual stress\n",
    "axes[1, 0].scatter(params[:, 2], outputs[:, 0], alpha=0.5, c=params[:, 0], cmap='viridis')\n",
    "axes[1, 0].set_title('Hatch Spacing vs. Residual Stress')\n",
    "axes[1, 0].set_xlabel('Hatch Spacing (mm)')\n",
    "axes[1, 0].set_ylabel('Residual Stress (MPa)')\n",
    "axes[1, 0].colorbar = plt.colorbar(axes[1, 0].collections[0], ax=axes[1, 0])\n",
    "axes[1, 0].colorbar.set_label('Laser Power (W)')\n",
    "\n",
    "# Scan angle vs. Porosity\n",
    "axes[1, 1].scatter(params[:, 3], outputs[:, 1], alpha=0.5, c=params[:, 1], cmap='plasma')\n",
    "axes[1, 1].set_title('Scan Angle vs. Porosity')\n",
    "axes[1, 1].set_xlabel('Scan Angle (°)')\n",
    "axes[1, 1].set_ylabel('Porosity (%)')\n",
    "axes[1, 1].colorbar = plt.colorbar(axes[1, 1].collections[0], ax=axes[1, 1])\n",
    "axes[1, 1].colorbar.set_label('Scan Speed (mm/s)')\n",
    "\n",
    "# Cooling rate proxy (P/sqrt(v)) vs Geometric accuracy\n",
    "cooling_rate_proxy = params[:, 0] / np.sqrt(params[:, 1]) # P/sqrt(v)\n",
    "scatter = axes[1, 2].scatter(cooling_rate_proxy, outputs[:, 2], alpha=0.5, c=params[:, 2], cmap='coolwarm')\n",
    "axes[1, 2].set_title('Cooling Rate Proxy vs. Geometric Accuracy')\n",
    "axes[1, 2].set_xlabel('P/sqrt(v) (W·s^(1/2)/mm^(1/2))')\n",
    "axes[1, 2].set_ylabel('Geometric Accuracy Ratio')\n",
    "axes[1, 2].colorbar = plt.colorbar(scatter, ax=axes[1, 2])\n",
    "axes[1, 2].colorbar.set_label('Hatch Spacing (mm)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Visualize Spatial Variations\n",
    "\n",
    "Now let's visualize how properties vary spatially across a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load spatial data for a single scan vector\n",
    "with h5py.File(dataset_path, 'r') as f:\n",
    "    # Get indices for the first scan vector\n",
    "    scan_vector_0 = f['train/scan_vectors'][0]\n",
    "    n_params = len(param_names)\n",
    "    \n",
    "    # Load all training data\n",
    "    all_inputs = f['train/inputs'][:]\n",
    "    all_outputs = f['train/outputs'][:]\n",
    "    all_coords = f['train/coordinates'][:]\n",
    "    \n",
    "    # Find points that match our scan vector\n",
    "    mask = np.all(np.isclose(all_inputs[:, :n_params], scan_vector_0), axis=1)\n",
    "    matched_coords = all_coords[mask]\n",
    "    matched_outputs = all_outputs[mask]\n",
    "    \n",
    "# Create contour plots to show spatial variations\n",
    "# First, reshape to 3D grid\n",
    "# Determine grid dimensions - assume cubic grid\n",
    "points_per_dim = int(np.cbrt(len(matched_coords)))\n",
    "\n",
    "# Extract x, y, z coordinates\n",
    "x = matched_coords[:, 0].reshape(points_per_dim, points_per_dim, points_per_dim)\n",
    "y = matched_coords[:, 1].reshape(points_per_dim, points_per_dim, points_per_dim)\n",
    "z = matched_coords[:, 2].reshape(points_per_dim, points_per_dim, points_per_dim)\n",
    "\n",
    "# Reshape outputs\n",
    "stress = matched_outputs[:, 0].reshape(points_per_dim, points_per_dim, points_per_dim)\n",
    "porosity = matched_outputs[:, 1].reshape(points_per_dim, points_per_dim, points_per_dim)\n",
    "accuracy = matched_outputs[:, 2].reshape(points_per_dim, points_per_dim, points_per_dim)\n",
    "\n",
    "# Create slices through the middle of the z-axis\n",
    "z_mid_idx = points_per_dim // 2\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot residual stress\n",
    "im0 = axes[0].contourf(x[:, :, z_mid_idx], y[:, :, z_mid_idx], stress[:, :, z_mid_idx], 50, cmap='hot')\n",
    "axes[0].set_title('Residual Stress Distribution')\n",
    "axes[0].set_xlabel('X (mm)')\n",
    "axes[0].set_ylabel('Y (mm)')\n",
    "plt.colorbar(im0, ax=axes[0], label='Stress (MPa)')\n",
    "\n",
    "# Plot porosity\n",
    "im1 = axes[1].contourf(x[:, :, z_mid_idx], y[:, :, z_mid_idx], porosity[:, :, z_mid_idx], 50, cmap='viridis')\n",
    "axes[1].set_title('Porosity Distribution')\n",
    "axes[1].set_xlabel('X (mm)')\n",
    "axes[1].set_ylabel('Y (mm)')\n",
    "plt.colorbar(im1, ax=axes[1], label='Porosity (%)')\n",
    "\n",
    "# Plot geometric accuracy\n",
    "im2 = axes[2].contourf(x[:, :, z_mid_idx], y[:, :, z_mid_idx], accuracy[:, :, z_mid_idx], 50, cmap='Blues')\n",
    "axes[2].set_title('Geometric Accuracy Distribution')\n",
    "axes[2].set_xlabel('X (mm)')\n",
    "axes[2].set_ylabel('Y (mm)')\n",
    "plt.colorbar(im2, ax=axes[2], label='Accuracy Ratio')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the scan vector parameters for reference\n",
    "scan_vector_info = {param: value for param, value in zip(param_names, scan_vector_0)}\n",
    "print(\"Scan vector parameters for this spatial visualization:\")\n",
    "for param, value in scan_vector_info.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PINN Model Training\n",
    "\n",
    "Now we'll train a Physics-Informed Neural Network (PINN) to predict the process outcomes based on scan parameters and spatial coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinn.model import PINN\n",
    "from pinn.physics import compute_physics_loss\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Load a subset of the data for demonstration\n",
    "with h5py.File(dataset_path, 'r') as f:\n",
    "    # Load inputs and outputs (limit to 5000 samples for faster training)\n",
    "    train_inputs = torch.tensor(f['train/inputs'][:5000], dtype=torch.float32)\n",
    "    train_outputs = torch.tensor(f['train/outputs'][:5000], dtype=torch.float32)\n",
    "    val_inputs = torch.tensor(f['val/inputs'][:1000], dtype=torch.float32)\n",
    "    val_outputs = torch.tensor(f['val/outputs'][:1000], dtype=torch.float32)\n",
    "    \n",
    "    # Extract scan vectors, coordinates, and time for physics loss\n",
    "    n_params = len(param_names)\n",
    "    train_scan_vectors = train_inputs[:, :n_params]\n",
    "    train_coords = train_inputs[:, n_params:n_params+3]\n",
    "    train_time = train_inputs[:, n_params+3:n_params+4]\n",
    "    \n",
    "    val_scan_vectors = val_inputs[:, :n_params]\n",
    "    val_coords = val_inputs[:, n_params:n_params+3]\n",
    "    val_time = val_inputs[:, n_params+3:n_params+4]\n",
    "\n",
    "# Set device (use GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create model\n",
    "model_config = config['model']\n",
    "model = PINN(\n",
    "    in_dim=model_config['input_dim'],\n",
    "    out_dim=model_config['output_dim'],\n",
    "    width=model_config['hidden_width'],\n",
    "    depth=model_config['hidden_depth']\n",
    ").to(device)\n",
    "\n",
    "# Move data to device\n",
    "train_inputs = train_inputs.to(device)\n",
    "train_outputs = train_outputs.to(device)\n",
    "train_scan_vectors = train_scan_vectors.to(device)\n",
    "train_coords = train_coords.to(device)\n",
    "train_time = train_time.to(device)\n",
    "\n",
    "val_inputs = val_inputs.to(device)\n",
    "val_outputs = val_outputs.to(device)\n",
    "val_scan_vectors = val_scan_vectors.to(device)\n",
    "val_coords = val_coords.to(device)\n",
    "val_time = val_time.to(device)\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(train_inputs, train_outputs, train_scan_vectors, train_coords, train_time)\n",
    "val_dataset = TensorDataset(val_inputs, val_outputs, val_scan_vectors, val_coords, val_time)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "# Define loss and optimizer\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Training parameters\n",
    "n_epochs = 100  # Reduced for demonstration\n",
    "physics_weight_schedule = np.linspace(0, 0.1, 30)  # Gradually increase physics weight\n",
    "physics_weight_schedule = np.pad(physics_weight_schedule, (0, n_epochs - len(physics_weight_schedule)), 'constant', constant_values=0.1)\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "data_losses = []\n",
    "physics_losses = []\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "print(\"Starting training...\")\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    epoch_data_loss = 0\n",
    "    epoch_physics_loss = 0\n",
    "    \n",
    "    # Set physics weight for this epoch\n",
    "    lambda_heat = physics_weight_schedule[epoch]\n",
    "    lambda_stress = physics_weight_schedule[epoch]\n",
    "    \n",
    "    # Training loop\n",
    "    for inputs, outputs, scan_vectors, coords, time in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predictions = model(inputs)\n",
    "        \n",
    "        # Data loss\n",
    "        data_loss = mse_loss(predictions, outputs)\n",
    "        \n",
    "        # Physics loss\n",
    "        physics_loss = compute_physics_loss(\n",
    "            model, \n",
    "            scan_vectors, \n",
    "            coords, \n",
    "            time, \n",
    "            config['material_properties'],\n",
    "            lambda_heat=lambda_heat,\n",
    "            lambda_stress=lambda_stress\n",
    "        )\n",
    "        \n",
    "        # Total loss\n",
    "        total_loss = data_loss + physics_loss\n",
    "        \n",
    "        # Backward and optimize\n",
    "        total_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # Clip gradients\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update epoch losses\n",
    "        epoch_loss += total_loss.item()\n",
    "        epoch_data_loss += data_loss.item()\n",
    "        epoch_physics_loss += physics_loss.item()\n",
    "    \n",
    "    # Calculate average losses\n",
    "    avg_loss = epoch_loss / len(train_loader)\n",
    "    avg_data_loss = epoch_data_loss / len(train_loader)\n",
    "    avg_physics_loss = epoch_physics_loss / len(train_loader)\n",
    "    \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, outputs, _, _, _ in val_loader:\n",
    "            predictions = model(inputs)\n",
    "            val_loss += mse_loss(predictions, outputs).item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader)\n",
    "    \n",
    "    # Update learning rate\n",
    "    scheduler.step(avg_val_loss)\n",
    "    \n",
    "    # Save best model\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'loss': avg_val_loss\n",
    "        }, models_dir / 'demo_best_model.pt')\n",
    "    \n",
    "    # Record losses\n",
    "    train_losses.append(avg_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    data_losses.append(avg_data_loss)\n",
    "    physics_losses.append(avg_physics_loss)\n",
    "    \n",
    "    # Print progress\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{n_epochs}], \"\n",
    "              f\"Loss: {avg_loss:.6f}, \"\n",
    "              f\"Data Loss: {avg_data_loss:.6f}, \"\n",
    "              f\"Physics Loss: {avg_physics_loss:.6f}, \"\n",
    "              f\"Val Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "print(f\"Training complete. Best validation loss: {best_val_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Visualize Training Progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.semilogy(train_losses, label='Training Loss')\n",
    "plt.semilogy(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (log scale)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot data loss vs physics loss\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.semilogy(data_losses, label='Data Loss')\n",
    "plt.semilogy(physics_losses, label='Physics Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss Component (log scale)')\n",
    "plt.title('Data Loss vs Physics Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Evaluate Model Accuracy\n",
    "\n",
    "Let's evaluate the model's prediction accuracy on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "with h5py.File(dataset_path, 'r') as f:\n",
    "    test_inputs = torch.tensor(f['test/inputs'][:], dtype=torch.float32).to(device)\n",
    "    test_outputs = torch.tensor(f['test/outputs'][:], dtype=torch.float32).to(device)\n",
    "\n",
    "# Load best model\n",
    "checkpoint = torch.load(models_dir / 'demo_best_model.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "# Make predictions\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_inputs)\n",
    "\n",
    "# Calculate metrics\n",
    "predictions_np = predictions.cpu().numpy()\n",
    "test_outputs_np = test_outputs.cpu().numpy()\n",
    "\n",
    "# Calculate R² for each output\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "\n",
    "r2_values = []\n",
    "rmse_values = []\n",
    "mae_values = []\n",
    "output_names = ['Residual Stress', 'Porosity', 'Geometric Accuracy']\n",
    "\n",
    "for i, name in enumerate(output_names):\n",
    "    r2 = r2_score(test_outputs_np[:, i], predictions_np[:, i])\n",
    "    rmse = np.sqrt(mean_squared_error(test_outputs_np[:, i], predictions_np[:, i]))\n",
    "    mae = mean_absolute_error(test_outputs_np[:, i], predictions_np[:, i])\n",
    "    \n",
    "    r2_values.append(r2)\n",
    "    rmse_values.append(rmse)\n",
    "    mae_values.append(mae)\n",
    "    \n",
    "    print(f\"{name}: R² = {r2:.4f}, RMSE = {rmse:.4f}, MAE = {mae:.4f}\")\n",
    "\n",
    "# Visualize predictions vs. actual values\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "for i, (name, ax) in enumerate(zip(output_names, axes)):\n",
    "    # Sample 1000 points for visualization\n",
    "    idx = np.random.choice(len(predictions_np), size=1000, replace=False)\n",
    "    ax.scatter(test_outputs_np[idx, i], predictions_np[idx, i], alpha=0.5)\n",
    "    \n",
    "    # Add perfect prediction line\n",
    "    min_val = min(test_outputs_np[:, i].min(), predictions_np[:, i].min())\n",
    "    max_val = max(test_outputs_np[:, i].max(), predictions_np[:, i].max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
    "    \n",
    "    ax.set_title(f'{name} Prediction (R² = {r2_values[i]:.4f})')\n",
    "    ax.set_xlabel('Actual Value')\n",
    "    ax.set_ylabel('Predicted Value')\n",
    "    ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Process-Property Maps\n",
    "\n",
    "Now that we have a trained model, we can use it to create detailed process-property maps to visualize how different parameter combinations affect outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a grid of process parameters\n",
    "# We'll focus on laser power and scan speed\n",
    "param_bounds = config['optimizer']['param_bounds']\n",
    "\n",
    "# Create grid for first two parameters (power and speed)\n",
    "p_range = np.linspace(param_bounds['P'][0], param_bounds['P'][1], 50)\n",
    "v_range = np.linspace(param_bounds['v'][0], param_bounds['v'][1], 50)\n",
    "P, V = np.meshgrid(p_range, v_range)\n",
    "\n",
    "# Get default values for other parameters\n",
    "default_params = {}\n",
    "for param in param_bounds:\n",
    "    if param not in ['P', 'v']:\n",
    "        default_params[param] = np.mean(param_bounds[param])\n",
    "\n",
    "# Create test points\n",
    "n_pts = P.shape[0] * P.shape[1]\n",
    "test_points = np.zeros((n_pts, len(param_names)))\n",
    "test_points[:, 0] = P.flatten()  # Power\n",
    "test_points[:, 1] = V.flatten()  # Speed\n",
    "\n",
    "# Fill in default values for other parameters\n",
    "for i, param in enumerate(param_names):\n",
    "    if i >= 2 and param in default_params:\n",
    "        test_points[:, i] = default_params[param]\n",
    "\n",
    "# Add dummy spatial coordinates (center point) and time\n",
    "coords = np.zeros((n_pts, 3))\n",
    "time = np.ones((n_pts, 1))\n",
    "\n",
    "# Create input tensor\n",
    "test_input = np.hstack([test_points, coords, time])\n",
    "test_input_tensor = torch.tensor(test_input, dtype=torch.float32).to(device)\n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(test_input_tensor)\n",
    "\n",
    "# Reshape predictions for contour plots\n",
    "pred_stress = predictions[:, 0].cpu().numpy().reshape(P.shape)\n",
    "pred_porosity = predictions[:, 1].cpu().numpy().reshape(P.shape)\n",
    "pred_accuracy = predictions[:, 2].cpu().numpy().reshape(P.shape)\n",
    "\n",
    "# Create contour plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Residual Stress\n",
    "contour0 = axes[0].contourf(P, V, pred_stress, 50, cmap='hot')\n",
    "axes[0].set_title('Residual Stress Map')\n",
    "axes[0].set_xlabel('Laser Power (W)')\n",
    "axes[0].set_ylabel('Scan Speed (mm/s)')\n",
    "plt.colorbar(contour0, ax=axes[0], label='Stress (MPa)')\n",
    "\n",
    "# Porosity\n",
    "contour1 = axes[1].contourf(P, V, pred_porosity, 50, cmap='viridis')\n",
    "axes[1].set_title('Porosity Map')\n",
    "axes[1].set_xlabel('Laser Power (W)')\n",
    "axes[1].set_ylabel('Scan Speed (mm/s)')\n",
    "plt.colorbar(contour1, ax=axes[1], label='Porosity (%)')\n",
    "\n",
    "# Geometric Accuracy\n",
    "contour2 = axes[2].contourf(P, V, pred_accuracy, 50, cmap='Blues')\n",
    "axes[2].set_title('Geometric Accuracy Map')\n",
    "axes[2].set_xlabel('Laser Power (W)')\n",
    "axes[2].set_ylabel('Scan Speed (mm/s)')\n",
    "plt.colorbar(contour2, ax=axes[2], label='Accuracy Ratio')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-objective Optimization\n",
    "\n",
    "Finally, let's use our trained PINN model with the NSGA-III algorithm to find Pareto-optimal process parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymoo.core.problem import Problem\n",
    "from pymoo.algorithms.moo.nsga3 import NSGA3\n",
    "from pymoo.util.ref_dirs import get_reference_directions\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.factory import get_sampling, get_crossover, get_mutation\n",
    "from pymoo.visualization.scatter import Scatter\n",
    "from pymoo.visualization.pcp import PCP\n",
    "\n",
    "class SurrogateProblem(Problem):\n",
    "    \"\"\"Multi-objective optimization problem using PINN surrogate\"\"\"\n",
    "    def __init__(self, model, param_bounds, param_names, device):\n",
    "        # Extract parameter bounds\n",
    "        self.n_var = len(param_bounds)\n",
    "        self.param_names = param_names\n",
    "        self.xl = np.array([param_bounds[p][0] for p in self.param_names])\n",
    "        self.xu = np.array([param_bounds[p][1] for p in self.param_names])\n",
    "        \n",
    "        # Set objectives (all three outcomes)\n",
    "        self.n_obj = 3\n",
    "        \n",
    "        # No constraints\n",
    "        self.n_constr = 0\n",
    "        \n",
    "        # Store the surrogate model\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        \n",
    "        super().__init__(\n",
    "            n_var=self.n_var,\n",
    "            n_obj=self.n_obj,\n",
    "            n_constr=self.n_constr,\n",
    "            xl=self.xl,\n",
    "            xu=self.xu\n",
    "        )\n",
    "    \n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        # Convert to tensor\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32, device=self.device)\n",
    "        \n",
    "        # Add dummy spatial coordinates and time\n",
    "        batch_size = x_tensor.shape[0]\n",
    "        coords = torch.zeros(batch_size, 3, device=self.device)  # Origin point\n",
    "        time = torch.ones(batch_size, 1, device=self.device)     # Final time step\n",
    "        \n",
    "        # Forward pass through the model\n",
    "        with torch.no_grad():\n",
    "            model_input = torch.cat([x_tensor, coords, time], dim=1)\n",
    "            predictions = self.model(model_input)\n",
    "        \n",
    "        # Extract objective values\n",
    "        objectives = predictions.cpu().numpy()\n",
    "        \n",
    "        # Negate geometric accuracy (we want to maximize it)\n",
    "        objectives[:, 2] = -objectives[:, 2]  # Negate for maximization\n",
    "        \n",
    "        # Set the objective values\n",
    "        out[\"F\"] = objectives\n",
    "\n",
    "# Set up problem\n",
    "problem = SurrogateProblem(\n",
    "    model=model,\n",
    "    param_bounds={name: param_bounds[name] for name in param_names},\n",
    "    param_names=param_names,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Get reference directions for 3 objectives\n",
    "ref_dirs = get_reference_directions(\"das-dennis\", 3, n_partitions=8)\n",
    "\n",
    "# Create the algorithm\n",
    "algorithm = NSGA3(\n",
    "    pop_size=100,\n",
    "    ref_dirs=ref_dirs,\n",
    "    sampling=get_sampling(\"real_random\"),\n",
    "    crossover=get_crossover(\"real_sbx\", prob=0.9, eta=15),\n",
    "    mutation=get_mutation(\"real_pm\", eta=20),\n",
    "    eliminate_duplicates=True\n",
    ")\n",
    "\n",
    "# Run the optimization (reduced generations for demo)\n",
    "n_gen = 10\n",
    "print(f\"Running optimization for {n_gen} generations...\")\n",
    "start_time = time.time()\n",
    "\n",
    "res = minimize(\n",
    "    problem=problem,\n",
    "    algorithm=algorithm,\n",
    "    termination=('n_gen', n_gen),\n",
    "    seed=42,\n",
    "    save_history=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Optimization complete in {elapsed_time:.2f} seconds\")\n",
    "print(f\"Number of solutions on Pareto front: {len(res.X)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Visualize Pareto Front"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract results\n",
    "X = res.X  # Parameter values\n",
    "F = res.F  # Objective values\n",
    "\n",
    "# Negate back the geometric accuracy for visualization\n",
    "F[:, 2] = -F[:, 2]\n",
    "\n",
    "# Create Pareto front visualization\n",
    "fig = plt.figure(figsize=(14, 12))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "scatter = ax.scatter(\n",
    "    F[:, 0],  # Residual stress\n",
    "    F[:, 1],  # Porosity\n",
    "    F[:, 2],  # Geometric accuracy\n",
    "    c=X[:, 0],  # Color by laser power\n",
    "    cmap='viridis',\n",
    "    s=50\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Residual Stress (MPa)')\n",
    "ax.set_ylabel('Porosity (%)')\n",
    "ax.set_zlabel('Geometric Accuracy')\n",
    "ax.set_title('Pareto Front of Optimal Solutions')\n",
    "\n",
    "# Add colorbar\n",
    "cbar = fig.colorbar(scatter, ax=ax, pad=0.1)\n",
    "cbar.set_label('Laser Power (W)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Parallel Coordinates Plot\n",
    "\n",
    "A Parallel Coordinates Plot helps visualize the relationships between process parameters and outcomes for multi-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a parallel coordinates plot\n",
    "fig, ax = plt.subplots(figsize=(16, 8))\n",
    "\n",
    "# Combine parameters and objectives\n",
    "param_obj_data = np.hstack([X, F])\n",
    "\n",
    "# Column names\n",
    "columns = list(param_names) + ['Stress', 'Porosity', 'Accuracy']\n",
    "\n",
    "# Normalize data for visualization\n",
    "param_obj_norm = np.zeros_like(param_obj_data)\n",
    "for i in range(param_obj_data.shape[1]):\n",
    "    min_val = np.min(param_obj_data[:, i])\n",
    "    max_val = np.max(param_obj_data[:, i])\n",
    "    if max_val > min_val:\n",
    "        param_obj_norm[:, i] = (param_obj_data[:, i] - min_val) / (max_val - min_val)\n",
    "    else:\n",
    "        param_obj_norm[:, i] = 0.5\n",
    "\n",
    "# Create parallel coordinates plot\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# Value for coloring (e.g., energy density)\n",
    "energy_density = X[:, 0] / (X[:, 1] * X[:, 2])  # P/(v*h)\n",
    "scaler = MinMaxScaler()\n",
    "colors = scaler.fit_transform(energy_density.reshape(-1, 1)).flatten()\n",
    "\n",
    "# Plot parallel coordinates\n",
    "x = np.arange(param_obj_norm.shape[1])\n",
    "for i, (y, c) in enumerate(zip(param_obj_norm, colors)):\n",
    "    ax.plot(x, y, color=cm.viridis(c), alpha=0.7)\n",
    "\n",
    "# Set x-ticks and labels\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(columns, rotation=45)\n",
    "\n",
    "# Set y-ticks and labels for parameters\n",
    "y_ticks = np.linspace(0, 1, 5)\n",
    "y_labels = []\n",
    "for i, col in enumerate(columns):\n",
    "    if i < len(param_names):  # Parameters\n",
    "        bounds = param_bounds[param_names[i]]\n",
    "        labels = [f\"{bounds[0]:.1f}\", \"\", f\"{np.mean(bounds):.1f}\", \"\", f\"{bounds[1]:.1f}\"]\n",
    "    else:  # Objectives\n",
    "        min_val = np.min(param_obj_data[:, i])\n",
    "        max_val = np.max(param_obj_data[:, i])\n",
    "        labels = [f\"{min_val:.2f}\", \"\", f\"{(min_val+max_val)/2:.2f}\", \"\", f\"{max_val:.2f}\"]\n",
    "    \n",
    "    y_labels.append(labels)\n",
    "\n",
    "# Set titles and labels\n",
    "ax.set_title('Parallel Coordinates Plot of Pareto-Optimal Solutions', fontsize=14)\n",
    "ax.grid(True)\n",
    "\n",
    "# Add a colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=cm.viridis, norm=plt.Normalize(energy_density.min(), energy_density.max()))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label('Energy Density (J/mm³)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Display Optimal Parameter Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame to display the Pareto-optimal solutions\n",
    "param_cols = {name: X[:, i] for i, name in enumerate(param_names)}\n",
    "obj_cols = {\n",
    "    'Residual_Stress': F[:, 0],\n",
    "    'Porosity': F[:, 1],\n",
    "    'Geometric_Accuracy': F[:, 2]\n",
    "}\n",
    "\n",
    "# Calculate energy density\n",
    "energy_density = X[:, 0] / (X[:, 1] * X[:, 2])  # P/(v*h)\n",
    "obj_cols['Energy_Density'] = energy_density\n",
    "\n",
    "# Create DataFrame\n",
    "results_df = pd.DataFrame({**param_cols, **obj_cols})\n",
    "\n",
    "# Display the top 10 solutions by different criteria\n",
    "print(\"Top 10 solutions with LOWEST RESIDUAL STRESS:\")\n",
    "display(results_df.sort_values('Residual_Stress').head(10))\n",
    "\n",
    "print(\"\\nTop 10 solutions with LOWEST POROSITY:\")\n",
    "display(results_df.sort_values('Porosity').head(10))\n",
    "\n",
    "print(\"\\nTop 10 solutions with HIGHEST GEOMETRIC ACCURACY:\")\n",
    "display(results_df.sort_values('Geometric_Accuracy', ascending=False).head(10))\n",
    "\n",
    "# Save results to CSV\n",
    "results_path = results_dir / 'pareto_optimal_solutions.csv'\n",
    "results_df.to_csv(results_path, index=False)\n",
    "print(f\"\\nResults saved to {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Selecting Optimal Process Parameters for Different Use Cases\n",
    "\n",
    "Based on our multi-objective optimization, we can identify optimal parameter sets for different application requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a few \"use cases\" with different priorities\n",
    "use_cases = {\n",
    "    'Structural': {'weights': [0.6, 0.2, 0.2]},  # Prioritize low stress\n",
    "    'Fluid_Handling': {'weights': [0.2, 0.6, 0.2]},  # Prioritize low porosity\n",
    "    'Precision_Parts': {'weights': [0.2, 0.2, 0.6]}  # Prioritize geometric accuracy\n",
    "}\n",
    "\n",
    "# Normalize the objectives for weighted scoring\n",
    "stress_norm = (F[:, 0] - F[:, 0].min()) / (F[:, 0].max() - F[:, 0].min())\n",
    "porosity_norm = (F[:, 1] - F[:, 1].min()) / (F[:, 1].max() - F[:, 1].min())\n",
    "# For accuracy, we want to maximize (not minimize), so invert the normalization\n",
    "accuracy_norm = 1 - (F[:, 2] - F[:, 2].min()) / (F[:, 2].max() - F[:, 2].min())\n",
    "\n",
    "norm_objectives = np.column_stack([stress_norm, porosity_norm, accuracy_norm])\n",
    "\n",
    "# Calculate weighted scores for each use case\n",
    "for case, details in use_cases.items():\n",
    "    weights = np.array(details['weights'])\n",
    "    scores = np.sum(norm_objectives * weights, axis=1)\n",
    "    best_idx = np.argmin(scores)  # Lower score is better\n",
    "    \n",
    "    print(f\"\\nOptimal parameters for {case} use case:\")\n",
    "    print(f\"  Scores weighted by {weights}\")\n",
    "    for i, param in enumerate(param_names):\n",
    "        print(f\"  {param}: {X[best_idx, i]:.4f}\")\n",
    "    print(\"\\nExpected outcomes:\")\n",
    "    print(f\"  Residual Stress: {F[best_idx, 0]:.2f} MPa\")\n",
    "    print(f\"  Porosity: {F[best_idx, 1]:.4f} %\")\n",
    "    print(f\"  Geometric Accuracy: {F[best_idx, 2]:.4f}\")\n",
    "    print(f\"  Energy Density: {energy_density[best_idx]:.4f} J/mm³\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary and Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated the complete workflow for LPBF process optimization using physics-informed neural networks:\n",
    "\n",
    "1. We generated synthetic data that mimics FEA simulation results for LPBF process outcomes.\n",
    "2. We explored the data to understand relationships between process parameters and outcomes.\n",
    "3. We trained a physics-informed neural network that incorporates both data-driven learning and physical constraints.\n",
    "4. We used the trained PINN as a surrogate model for multi-objective optimization using NSGA-III.\n",
    "5. We visualized the Pareto-optimal solutions and identified optimal parameter sets for different use cases.\n",
    "\n",
    "This approach offers several key advantages:\n",
    "\n",
    "- **Efficiency**: Using a PINN surrogate avoids running thousands of computationally expensive FEA simulations.\n",
    "- **Physics-based**: The model respects physical laws even in regions without data points.\n",
    "- **Multi-objective**: The optimization balances competing objectives like minimizing stress and porosity while maximizing geometric accuracy.\n",
    "- **Interpretable**: The process-property maps provide insights into how different parameters affect outcomes.\n",
    "\n",
    "The optimal process parameters identified can be used to guide real LPBF manufacturing processes, potentially saving time and resources while improving part quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}